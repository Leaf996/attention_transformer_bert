# attention_transformer_bert
- attention_transformer_bert

# paper list
- sequence-to-sequence-learning-with-neural-networks(**Seq2Seq**)
    - [sequence-to-sequence-learning-with-neural-networks(pdf)][11]
    - [sequence-to-sequence-learning-with-neural-networks(md)][12]
- Neural_Machine_Translation_by_Jointly_Learning_to_Align_and_Translate(**First Attention, additive attention**)
    - [Neural_Machine_Translation_by_Jointly_Learning_to_Align_and_Translate(pdf)][3]
    - [Neural_Machine_Translation_by_Jointly_Learning_to_Align_and_Translate(md)][4]
- Effective Approaches_to_Attention-based_Neural_Machine_Translation(**multiple attention**)
    - [Effective Approaches_to_Attention-based_Neural_Machine_Translation(pdf)][5]
    - [Effective Approaches_to_Attention-based_Neural_Machine_Translation(md)][6]
- Long_Short_Term_Memory_Networks_for_Machine_Reading(**self-attention**)
    - [Long_Short_Term_Memory_Networks_for_Machine_Reading(pdf)][7]
    - [Long_Short_Term_Memory_Networks_for_Machine_Reading(md)][8]
- Attention_is_all_you_need(**transformer**)
    - [Attention_is_all_you_need(pdf)][9]
    - [Attention_is_all_you_need(md)][10]
- Listen, Attend and Spell(**attention end2end asr**)
    - [Listen, Attend and Spell(pdf)][1]
    - [Listen, Attend and Spell(md)][2]

# Bert Related
- Blog
    - [NLP's ImageNet moment has arrived][13]
    - [Open Sourcing BERT: State-of-the-Art Pre-training for Natural Language Processing][15]
- Reference Repo
    - [bert][14]
    - [transformers][16]

# awesome repo
- [bertviz][17]


[1]:pdf/Listen_attend_spell.pdf
[2]:md/Listen_attend_spell.md
[3]:pdf/Neural_Machine_Translation_by_Jointly_Learning_to_Align_and_Translate.pdf
[4]:md/Neural_Machine_Translation_by_Jointly_Learning_to_Align_and_Translate.md
[5]:pdf/Effective_Approaches_to_Attention-based_Neural_Machine_Tr.pdf
[6]:md/Effective_Approaches_to_Attention-based_Neural_Machine_Tr.md
[7]:pdf/Long_Short_Term_Memory_Networks_for_Machine_Reading.pdf
[8]:md/Long_Short_Term_Memory_Networks_for_Machine_Reading.md
[9]:pdf/Attention_is_all_you_need.pdf
[10]:md/Attention_is_all_you_need.md
[11]:pdf/sequence-to-sequence-learning-with-neural-networks.pdf
[12]:md/sequence-to-sequence-learning-with-neural-networks.md
[13]:https://ruder.io/nlp-imagenet/
[14]:https://github.com/google-research/bert
[15]:http://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html
[16]:https://github.com/huggingface/transformers
[17]:https://github.com/jessevig/bertviz