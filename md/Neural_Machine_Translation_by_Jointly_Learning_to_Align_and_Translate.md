## Neural_Machine_Translation_by_Jointly_Learning_to_Align_and_Translate
### Questions
- how to process variable length(input and output) ?
- when to stop ?
- how to understand seq2seq ?


### Key Concepts
- fixed-length vector vs. variable-length vector
- soft-alignment vs. hard-alignment
- first attention(Bahdanau attention)


### Seq2Seq
- [系统了解Encoder-Decoder 和 Seq2Seq及其关键技术][7]
- [真正的完全图解Seq2Seq Attention模型][8]
- [两种常见Seq2Seq的原理及公式][9]
- [seq2seq学习笔记][10]
- [Sequence to Sequence Networks][11]
- [Attention Mechanism][12]
- [Attention Mechanism: Benefits and Applications][13]
- [Attention? Attention!][14]
- [The Unreasonable Effectiveness of Recurrent Neural Networks][15]

### NMT Reference Repo
- [attention-nmt][1]
- [RNN-NMT][2]
- [mlpnlp-nmt][3]
- [attention-nmt][4]
- [NMT][5]
- [RNNSearch][6]


### Seq2Seq Reference Repo
- [seq2seq][16]
- [seq2seq with keras][17]
- [seq2seq with pytorch][18]
- [**pytorch-seq2seq**][19]
- [IBM pytorch-seq2seq][20]


[1]:https://github.com/pemywei/attention-nmt
[2]:https://github.com/VectorFist/RNN-NMT
[3]:https://github.com/mlpnlp/mlpnlp-nmt
[4]:https://github.com/carrie0307/attention-nmt
[5]:https://github.com/vycezhong/NMT
[6]:https://github.com/xwgeng/RNNSearch
[7]:https://zhuanlan.zhihu.com/p/114933655
[8]:https://zhuanlan.zhihu.com/p/40920384
[9]:https://zhuanlan.zhihu.com/p/70880679
[10]:https://blog.csdn.net/Jerr__y/article/details/53749693
[11]:https://towardsdatascience.com/nlp-sequence-to-sequence-networks-part-2-seq2seq-model-encoderdecoder-model-6c22e29fd7e1
[12]:https://blog.floydhub.com/attention-mechanism/
[13]:https://www.saama.com/attention-mechanism-benefits-and-applications/
[14]:https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html#born-for-translation
[15]:http://karpathy.github.io/2015/05/21/rnn-effectiveness/
[16]:https://github.com/google/seq2seq
[17]:https://github.com/farizrahman4u/seq2seq
[18]:https://github.com/keon/seq2seq
[19]:https://github.com/bentrevett/pytorch-seq2seq
[20]:https://github.com/IBM/pytorch-seq2seq