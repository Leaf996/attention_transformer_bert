## Attention_is_all_you_need
### Questions
- How to understand resolution reducition(Background section) ?
- How to do inference ?


### Key Concepts
- transformer(**First Blood**)
- Sequence transduction
- convolution entirely
- scaled dot-product attention
- multi-head attention
- parameter-free position representation
- global dependencies
- self-attention
- **query, key, values**


### Reference Repo
- [transformer-tensorflow][1]
- [**tensor2tensor**][8]
- [annotated-transformer][9]
- [sockeye][10]


### ASR Transformer
- [Speech-Transformer][2]
- [Speech-Transformer][3]


### Application
- reading comprehension
- abstractive summarization
- textual entailment


### Reference Blog
- [Attention学习_强化篇][4]
- [**The Illustrated Transformer**][5]
- [Visualizing A Neural Machine Translation Model][6]
- [**The Annotated Transformer**][7]
- [**理解语言的 Transformer 模型**][11]
- [**Glossary of Deep Learning: Word Embedding**][12]
- [Transformer: A Novel Neural Network Architecture for Language Understanding][13]


[1]:https://github.com/lilianweng/transformer-tensorflow
[2]:https://github.com/kaituoxu/Speech-Transformer
[3]:https://github.com/foamliu/Speech-Transformer
[4]:https://mp.weixin.qq.com/s/syOf6BpHP3x-DKI584YM3g
[5]:https://jalammar.github.io/illustrated-transformer/
[6]:https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/
[7]:http://nlp.seas.harvard.edu/2018/04/03/attention.html
[8]:https://github.com/tensorflow/tensor2tensor
[9]:https://github.com/harvardnlp/annotated-transformer
[10]:https://github.com/awslabs/sockeye
[11]:https://www.tensorflow.org/tutorials/text/transformer#training_and_checkpointing
[12]:https://medium.com/deeper-learning/glossary-of-deep-learning-word-embedding-f90c3cec34ca
[13]:https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html