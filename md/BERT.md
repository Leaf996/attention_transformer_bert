# BERT : Bidirectional Encoder Representations from Transformers
## Question
- TODO
## Key Concepts
- **Pre-Training**
- **Word Embeddings**
- General Language Representations
- **MLM**  : Masked Language Model
- Contextual Representations
- Conditional Language Model
- Denoising Auto-encoders
## Feature-based vs. Fine-tuning
- Feature-Based : parameters fixed, ELMo
- Fine-Tuning : parameters un-fixed, OpenAI GPT
## MLM vs. NSP
- MLM : Masked Language Model
- NSP : Next Sentence Prediction